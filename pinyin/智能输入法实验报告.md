# 智能拼音输入法实验报告

**2013xxxxx xx 人工智能学院**

<font color="red">诚信声明：GUI模式的实现中，前端部分参考了GitHub上的代码[online-ime](https://github.com/jtzcode/online-ime)</font>

## 一. 使用指南

首先安装依赖

```
cd pinyin
pip install -r requirements.txt
```

智能拼音输入法提供两种模式——命令行模式和GUI模式。

#### 命令行模式

```
python code/main.py
```

根据提示在终端输入即可

![image-20220722153607652](C:\Users\17543\AppData\Roaming\Typora\typora-user-images\image-20220722153607652.png)

#### GUI模式

```
python gui/server.py
```

然后在浏览器输入

```
http://localhost:5000/
```

即可运行

![image-20220723194615618](C:\Users\17543\AppData\Roaming\Typora\typora-user-images\image-20220723194615618.png)

## 二. 实现功能

本项目实现了智能拼音输入法，可以根据输入的拼音字符串生成多个推荐的汉字（词组）。

#### 参数介绍

智能拼音输入法接受如下的参数自定义：

- 在对输入的拼音字符的处理方面，
  - 输入的拼音字符是否都是全拼，对应的参数是`is_quanpin`，默认值为False。比如，如果输入的字符是**<font color="green">`pinyin`</font>**，那么`is_quanpin`就可以设置为True，但如果输入的字符是**<font color="green">`piny`</font>**或者**<font color="green">`py`</font>**，那么`is_quanpin`应该设置为False。
  - 输入的拼音字符是否带分割符，对应的参数是`has_separator`，默认值为False。当设置为True时，拼音划分函数会对拼音字符串有分割符**<font color="green">`'`</font>**的地方进行强制划分。比如，如果输入的字符是**<font color="green">`xi'an`</font>**，且`has_separator=True`时，则划分函数不会将字符划分为**<font color="green">`xian`</font>**。
- 在对命令行模式输出内容的处理方面，

  - 输出中是否带有分割相关信息，对应的参数是`code.main.py`中`cli()`中的`show_cut`，default为False，如果设置为True则会输出分割相关的信息。
  - 输出中是否带有概率相关信息，对应的参数是`code.main.py`中`cli()`中的`show_prob`default为False，如果设置为True则会在每个推荐的汉字序列后显示该汉字出现的概率相关数据。数值越大说明出现概率越大，更被输入法推荐。
  - 推荐的汉字（词组）个数，对应的参数是`limit`，default为10
  - 当`show_cut=True, show_prob=True, limit=10`时，输出如下：

![image-20220723122633399](C:\Users\17543\AppData\Roaming\Typora\typora-user-images\image-20220723122633399.png)

- 在HMM模型的训练方面
  - 可以自定义首字母缩写与全拼的大小比例，对应的参数是`code.hmm.py`中`gen_emission_matrix()`中的`initial_weight`，default值为0.1。

#### 项目结构介绍

智能拼音输入法的项目结构如下：


```python
.
├── code
│   ├── cut.py # 拼音划分相关代码
│   ├── greedy.py # 生成最优汉字（词组）推荐代码
│   ├── hmm.py # hmm学习训练代码
│   └── main.py # 包含主函数和命令行模式代码
├── data
│   ├── global_wordfreq.release.txt # 词频语料数据
│   ├── possible_pinyin.txt # 所有可能的全拼
│   └── generated # hmm生成的数据
│       ├── hmm_emission_freq_matrix.json
│       ├── hmm_emission_matrix.json # 估计观测概率矩阵
│       ├── hmm_emission_reversed_freq_matrix.json
│       ├── hmm_emission_reversed_matrix.json # 估计观测概率矩阵的逆矩阵
│       ├── hmm_start_freq.json
│       ├── hmm_start_vector.json # 初始状态概率向量
│       ├── hmm_transition_freq_matrix.json
│       └── hmm_transition_matrix.json # 状态转移概率矩阵
├── gui # GUI模式实现
├── 201300005 俞睿 智能输入法实验报告.md
└── requirements.txt
```

## 三. 使用原理

本小节将介绍智能拼音输入法的核心算法：HMM模型的学习与预测。

#### 3.1 HMM模型的训练

在项目的语境中，观测序列为拼音列表，状态序列为汉字列表，举例：

![image-20220723180845602](C:\Users\17543\AppData\Roaming\Typora\typora-user-images\image-20220723180845602.png)

HMM（隐马尔可夫模型）描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测值，从而做种产生观测序列的过程。相关背景在课上介绍过，下介绍对状态转移矩阵、观测概率（发射概率）矩阵、初始的状态概率的计算公式：

- 状态转移概率矩阵$\mathbf{a}$，其中，$\displaystyle a_{i,j} = \frac{A_{i,j}}{\sum_u A_{i,u}}$，$A_{i,j}$表示前一个汉字为$i$，后一个汉字为$j$的频数

- 估计观测概率矩阵$\mathbf b$，其中，$\displaystyle b_{j,k} = \log\frac{B_{j,k}}{\sum_{v}B_{j,v}}$，$B_{j,k}$表示汉字$j$的拼音为$k$的频数
- 初始状态概率向量$\pi$，其中$\displaystyle\pi_i = \log\frac{C_i}{\sum_{i}C_i}$，$C_i$是汉字$i$作为字符串第一个字符出现的次数

#### 3. 2 Viterbi算法

Viterbi算法可以根据HMM模型和观测到的状态拼音序列，预测最大概率出现的汉字序列。

- 输入: 
  - HMM模型$\lambda = (\mathbf A, \mathbf B, \pi)$
  - 观测到的拼音序列$\mathbf{O} = (o_1, o_2,\cdots,o_{T})$，举例：`('ni', 'hao')`
- 初始化: $\delta_1(i) = \pi_ib_i(o_1), \Psi_1(i)=0, i=1,2,\cdots,Q$，其中，$\delta_1(i), \Psi_1(i)$表示拼音$o_1$对应的汉字$i$的$\delta,\Psi$值
- 递推: $\delta_t(i) = \max_{1\leq j\leq Q}\delta_{t-1}(j)\times a_{j,t}\times b_i(o_t), i=1,2,\cdots,Q, t = 2,\cdots,T$
- 终止：$P^* = \max_{1\leq i \leq Q}\delta_T(i), i^*_T=\arg\max_{1\leq j\leq Q}\delta_{t-1}(j)a_{j,i}$
- 最优路径回溯：$i^{*}_t=\Psi_{t+1}(i^*_{t+1}), t=T-1,\cdots,1$
- 获得最优路径：$\mathbf I^* = (i_1^*, \cdots,i^{*}_T)$ 

## 四. 代码实现

### 4.1 对拼音字符串的处理

#### 4.1.1 拼音划分

在`code.cut.py`中的`pinyin_cut()`函数中实现了对拼音的划分。

`pniyin_cut()`的输入为：

- `input_str`：用户输入的字符串，考虑到的可能情况如下：
  - 全拼
  - 拼音首字母（包括**<font color="green">`zh`</font>**，**<font color="green">`ch`</font>**，**<font color="green">`sh`</font>**）
  - 分隔符号**<font color="green">`'`</font>**
- `is_quanpin`：输入是否一定是全拼，default为True
- `has_separator`：输入是否有分隔符，default为False

输出为：

- `ret_list`：包含所有可能的拼音分割情况的列表

举例：`pinyin_cut("nihao") == [["ni", "hao"], ["ni", "h", "ao"]]`

`pinyin_cut()`运用了动态规划的思想，大体思路为：设置一个`pinyin_set`，包含所有可能的拼音。从头遍历`input_str`，如果遍历到的子字符串`input_str[:i]`在`pinyin_set`中，则记录划分结果，对剩余部分`input_str[i:]`继续递归调用`pinyin_cut()`，直到完成遍历。

具体代码实现如下：

```python
def pinyin_cut(input_str: str, is_quanpin=True, has_separator=False) -> list:
    # 如果是全拼的话，pinyin_set就是所有可能的全拼的集合
    if is_quanpin:
        pinyin_set = quanpin_set
    # 否则，pinyin_set就是全拼集合与声母集合的并集
    else:
        shengmu = {'b','p','m','f','d','t','n','l','g','k','h','j','q','x','zh','ch','sh','r','z','c','s','y','w'}
        pinyin_set = quanpin_set.union(shengmu)
    
    ret_list = []
    # 如果允许使用分割符号「'」进行分割的话，按照分割符分割，输出res_list
    if has_separator and '\'' in input_str:
        separated_strs = input_str.split('\'')
        # print("separated_strs", separated_strs)
        # 对于分割后的每个子字符串，递归调用pinyin_cut再进行分割
        for i in range(len(separated_strs)):
            sub_list = pinyin_cut(separated_strs[i], is_quanpin=is_quanpin, has_separator=False)
            # print("sub_list", sub_list)
            # 如果两个列表都非空，则进行叠加
            if ret_list and sub_list:
                new_list = []
                for e1 in ret_list:
                    for e2 in sub_list:
                        new_list.append(e1.copy()+e2.copy())
                ret_list = new_list
            elif not ret_list:
                ret_list = sub_list
            # print("ret_list", ret_list)
        return ret_list
    else:
        # 如果输入的字符串就在pinyin_set中
        if input_str in pinyin_set:
            ret_list = [[input_str]]
        # 否则递归调用
        for i in range(1, len(input_str)):
            if input_str[:i] in pinyin_set:
                sub_list = pinyin_cut(input_str[i:], is_quanpin=is_quanpin, has_separator=False)
                for e in sub_list:
                    e.insert(0, input_str[:i])
                ret_list += sub_list
    return ret_list
```

#### 4.1.2 拼音规范化

在我们日常拼音输入中，常常存在一些错误，比如说输入“略”的拼音时，我们倾向于输入**<font color="green">`lue`</font>**，但规范的输入应该是**<font color="green">`lve`</font>**。在拼音中有如下规定：除了**<font color="green">`j`</font>**，**<font color="green">`q`</font>**，**<font color="green">`x`</font>**，**<font color="green">`y`</font>**后应该接**<font color="green">`ue`</font>**，其余声母后均应是**<font color="green">`ve`</font>**。我们通过`normalize()`函数规范错误的输入。

`normalize()`的输入为：已经划分好的拼音序列列表；输出为：规范化后的拼音列表。

举例：`normalize(["lue"]) == [["lve"]]`

具体代码实现如下：

```python
def normalize(pinyins):
    for py in pinyins:
        for i in range(len(py)):
            print("py: ", py)
            if 'ue' in py[i] and py[i][0] not in ['j', 'q', 'x', 'y']:
                print(py[i], py[i].replace('u', 'v'))
                py[i] = py[i].replace('u', 'v')
    return pinyins
```

#### 4.1.3 综合处理

在`code.cut.py`中的`cut()`函数中调用了以上两个函数，实现了对拼音字符串的处理。

```python
def cut(pinyins, is_quanpin=True, has_separator=False):
    return normalize(pinyin_cut(pinyins, is_quanpin, has_separator))
```

### 4.2 HMM的训练与预测

在`code.hmm.py`中，使用北京语言大学 BCC 语料库`global_wordfreq.release.txt`，训练生成了HMM模型。

#### 4.2.1 数据的读取与保存

为了读取`.txt`文件中的数据，保存并读取生成的模型，在`code.hmm.py`中我实现了三个函数：

- `dict2json()`，将字典保存为json文件：
  - 它的输入为：
    - `dictionary`：待保存的矩阵或向量（数据类型为dict）
    - `path`：所应保存的位置
- `json2dict()`，将json文件转换为字典
  - 它的输入为：
    - `path`：矩阵或向量保存的位置
  - 它的输出为：
    - `ret_dict`：读出的矩阵或向量（数据类型为dict）
- `read_wordfreq()`，打开`data.global_wordfreq.release.txt`，读入词汇频率数据
  - 它的输入为：
    - `word, freq`的generator

`dict2json()`与`json2dict()`的实现较为简单，故不在这里详细介绍，代码可见`code.hmm.py`。`read_wordfreq()`的代码实现如下：

其中使用`yield`，使得每次调用返回一个生成器；为了正常读取汉字，使用了ANSI的encoding。

```python
def read_wordfreq():
    # output: word-freq的generator
    with open(wordfreq_filepath, 'r', encoding='ANSI') as f:
        for line in f:
            try:
                word, freq = line.split()
                # 判断字符串word中的每个字符是否都是中文
                if all('\u4e00' <= ch <= '\u9fff' for ch in word):
                    yield word, int(freq)
            except:
                print(line)
```

#### 4.2.2  HMM模型的训练

##### 代码实现

我实现了三个函数，分别用于生成HMM模型中的状态转移概率矩阵、估计观测概率矩阵与初始状态概率向量，具体实现见`code.hmm.py`：

- 在`gen_transition_matrix()`中，

  - 生成状态频数矩阵$\mathbf{A}$并保存到`data/generated/hmm_transition_freq_matrix.json`，其中，$A_{i,j}$表示前一个汉字为$i$，后一个汉字为$j$的频数

  - 生成状态转移概率矩阵$\mathbf{a}$并保存到`data/generated/hmm_transition_matrix.json`，其中，$\displaystyle a_{i,j} = \frac{A_{i,j}}{\sum_u A_{i,u}}$
- 在`gen_emission_matrix()`中，

  - 它的输入为：
    - `initial_weight`，在估计观测概率预测中缩拼与全拼权重的比例，default为0.1
  - 生成估计观测频数矩阵$\mathbf{B}$并保存`data/generated/hmm_emission_freq_matrix.json`，其中，$B_{j,k}$表示汉字$j$的拼音为$k$的频数
  - 生成估计观测概率矩阵$\mathbf b$并保存到`data/generated/hmm_emission_matrix.json`，其中，$\displaystyle b_{j,k} = \log\frac{B_{j,k}}{\sum_{v}B_{j,v}}$
  - 生成它们的转置矩阵$\mathbf{B}^\top$与$\mathbf b^\top$，并分别保存到`"data/generated/hmm_emission_reversed_freq_matrix.json"`与`data/generated/hmm_emission_reversed_matrix.json`
- 在`gen_start_vector()`中，
  - 生成初始状态概率向量$\pi$并保存到，$\displaystyle\pi_i = \log\frac{C_i}{\sum_{i}C_i}$，其中，$C_i$是汉字$i$作为字符串第一个字符出现的次数


##### 训练结果

生成的状态转移概率矩阵$\mathbf{a}$（局部）为：

```json
{
    "一": {
        "一": -12.718060635939068,
        "丁": -8.25123123284051,
        "七": -11.601528117037388,
        "万": -10.968040772883777,
        "丈": -10.77552544790028,
        "三": -11.45257570141805,
        "上": -10.069113023709411,
        "下": -3.071542961505963,
        "不": -6.6698023248011316,
        "专": -9.944146141979724,
        "世": -6.473001113081323,
        "丘": -10.385962537685463,
    },
}
```

生成估计观测概率矩阵$\mathbf b$（局部）为：

```json
{
    "一": {
        "y": -2.3978952727983702,
        "yi": -0.09531017980432477
    },
    "万": {
        "m": -9.344173012668811,
        "mo": -7.041587919674766,
        "w": -2.398857946112073,
        "wan": -0.09627285311802737
    },
}
```

生成估计观测概率矩阵的转置矩阵$\mathbf b^\top$（局部）为：

```json
{
	"ba": {
        "丷": -0.0953101798043249,
        "仈": -0.09531017980432477,
        "八": -0.0953101798043249,
        "叐": -0.0953101798043249,
        "叭": -0.09531017980432477,
        "吧": -0.0953101798043249,
    },
}
```

生成的初始状态概率向量$\pi$（局部）为：

```json
{
    "一": -5.081293678906249,
    "丁": -9.192433659783104,
    "丂": -19.34085746030175,
    "七": -10.159009715890134,
    "丄": -16.747217610377284,
    "丅": -16.301496324358553,
    "丆": -19.25047339883348,
    "万": -8.7175005342102,
    "丈": -9.453436118761804,
}
```

##### 特别注意

在实现中，概率很容易趋近于0，可能造成数据的损失。比如说，在生成初始状态概率向量时，由于可选的初始向量过多，所以概率有时到了$e^{-10}-e^{-20}$的数量级。因此在矩阵的存储中，我们将概率取对，由于对数函数单增，如此操作在不破坏概率的大小关系的前提下能够更好地保存数据。

#### 4.2.3 HMM模型的预测

在`code.greedy.py`中，我实现了一种Greedy+剪枝的算法，基于HMM模型和观测到的状态拼音序列，预测最大概率出现的汉字序列，在合理是时间与空间复杂度内取得了较好的预测效果。

##### 算法选择

在算法选择中，我没有使用Viterbi算法，这是因为经典Viterbi算法只能得出一个最优路径，而输入法需要生成多个推荐的汉字（词组）。解决方法有两个：一是得出一个最优和多个局部最优路径，但在有些情况下，此处的局部最优路径与最优的k个路径相差较大，所以放弃了这种方法。二是使用改良后的Viterbi算法算出最优的k个路径（见引用1），但在阅读论文后，我发现这种方法实现较为困难，且时间复杂度为传统Viterbi算法的$\Theta(k)$倍，所以也放弃了这种方法。最后选择参考Viterbi算法对最优路径的判断标准，实现了一种Greedy+剪枝的算法。

##### 算法实现

在`code.greedy.py`的`greedy()`函数中，实现了一种Greedy+剪枝的算法。

`greedy()`的输入为：

- `pinyins`，划分好的拼音序列（数据类型为tuple），例如，`("ni", "hao")`。
- `limit`，输出的推荐的汉字（词组）的个数，default为10。

`greedy()`的输出为：

- `limit`个推荐的汉字（词组）与概率的元组`(word, freq)`构成的列表，按照出现的概率排序

举例，

```python
print(greedy(("ni", "hao"), 10))

# 输出：[('你好', -7.397965853621274), ('尼好', -17.139621286841127), ('倪浩', -17.214330796007292), ('倪豪', -17.791218170451376), ('倪好', -18.266111784991303), ('尼豪', -18.65686343070861), ('妮好', -19.22078402804663), ('倪昊', -19.713030767927624), ('倪皓', -20.40617794848757), ('逆儫', -inf)]
```

`greedy()`函数中对每个预测汉字结点的概率计算规则与Viterbi相同，即

- 初始化: $\delta_1(i) = \pi_ib_i(o_1),i=1,2,\cdots,Q$，其中，$\delta_1(i), $表示拼音$o_1$对应的汉字$i$的$\delta$值
- 递推: $\delta_t(i) = \max_{1\leq j\leq Q}\delta_{t-1}(j)\times a_{j,t}\times b_i(o_t), i=1,2,\cdots,Q, t = 2,\cdots,T$

由于概率都被取了对数，所以计算中的乘法改为加法。

另外，为了下一步中不同拼音序列间的比较（比如说，**<font color="green">`nihao`</font>**的划分有`["ni", "hao"]`和`["ni","ha","o"]`，将它们都通过`greedy()`预测得到`limit`个推荐汉字序列，如何综合两者的结果输出最后的汉字序列），在递推的概率计算上还减去了拼音的个数的对数。

所以实际的计算规则为：

- 初始化: $\log(\delta_1(i)) = \log(\pi_i)+\log(b_i(o_1)),i=1,2,\cdots,Q$，其中，$\delta_1(i), $表示拼音$o_1$对应的汉字$i$的$\delta$值
- 递推: $\displaystyle \log(\delta_t(i)) =-\log T+ \max_{1\leq j\leq Q}\left(\log\left (\delta_{t-1}(j)\right)+\log\left(a_{j,t}\right)+\log\left(b_i(o_t)\right)\right), i=1,2,\cdots,Q, t = 2,\cdots,T$

为了方便查找，从观测概率矩阵的转置矩阵$\mathbf b^\top$中查找$b_i(o_t)$。

用dict数据类型的`possible_routes`存储最优的`limit`个路径。遍历拼音序列`pinyins`的每一个拼音。对于第一个拼音`pinyins[0]`，按照上述初始化规则计算每个汉字的$\delta_1(i)$，选取其中最大的`limit`个存储到`possible_routes`。接下来，按照递推规则计算`pinyin[i]`与每个汉字的$\delta_t(i)$，但此处的$\max$的计算是从`possible_routes`中的路径取。也就是说，要计算$\text{limit}\times\text{limit}$条路径对应的$\delta_t(i)$，取其中最大的`limit`条，替代原来的`possible_routes`。结束遍历后，输出排序好的`possible_routes`作为推荐的结果。

具体代码实现如下：

```python
def greedy(pinyins, limit):
    # initialization
    py = pinyins[0]
    possible_routes = {}
    for ch in emission_matrix_T[py]:
        prob = emission_matrix_T[py][ch]
        if ch in start_vector:
            prob += start_vector[ch]
        else:
            prob += float('-inf')
        possible_routes = update_routes(ch, prob, possible_routes, limit)

    # recursion
    for i in range(1, len(pinyins)):
        last_possible_routes = possible_routes.copy()
        possible_routes = {}
        py = pinyins[i]
        for ch in emission_matrix_T[py]:
            for word in last_possible_routes:
                last_ch = word[-1]
                prob = last_possible_routes[word] + emission_matrix_T[py][ch]
                if last_ch in transition_matrix and ch in transition_matrix[last_ch]:
                    prob += transition_matrix[last_ch][ch]
                else:
                    prob += float('-inf')
                prob += log(len(pinyins)) # 平衡不同长度的划分
                possible_routes = update_routes(word + ch, prob, possible_routes, limit)
    return sorted(possible_routes.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)
```

##### 推荐实现

在`code.greedy.py`的`gen_hanzi()`函数中，实现了推荐算法。

`gen_hanzi()`的输入为：

- `pinyin_list`，`cut()`生成的拼音划分列表，例如`[["ni", "hao"]，["ni","ha","o"]]`
- `limit`，输出的推荐的汉字（词组）的个数，default为10。

`gen_hanzi()`的输出为：

- `result_wordlist`，综合的`limit`个推荐的汉字（词组）与概率的元组`(word, freq)`构成的列表，按照出现的概率排序

实现方法为：选择所有推荐中概率最大的`limit`个汉字序列返回。

具体代码实现如下：

```python
def gen_hanzi(pinyin_list, limit=10):
    result_wordlist = list([])
    for pinyins in pinyin_list:
        wordlist = greedy(pinyins, limit)
        result_wordlist = update_wordlist(wordlist, result_wordlist, limit)
    return result_wordlist
```

### 4.3 用户交互

#### 4.3.1 输入法函数

在`code.main.py`的`input_method()`函数中，实现了输入法的总函数：

```
def input_method(input_str, is_quanpin=True, has_separator=False, limit=10):
    cut_res = cut(input_str, is_quanpin, has_separator)
    hanzi_res = gen_hanzi(cut_res, limit)
    return cut_res, hanzi_res
```

智能拼音输入法提供两种与用户交互的模式——命令行模式和GUI模式（未实现）

#### 4.3.2 命令行模式

具体实现在`code.main.py`的`cli()`函数中。

#### 4.3.3 GUI模式

具体实现在`gui`中。

## 五 自我评价

#### 优点

- 推荐的汉字较符合人们的输入习惯，时间和空间开销较小
- 实现了HMM模型的训练，创新地实现了greedy+剪枝的预测算法
- 实现了命令行模式，有较友好的用户交互

#### 缺点

- 语料数据集稍显过时，pypinyin的拼音转化不完全准确，对预测结果的准确性产生了一定影响
- Web前端过于简陋，对用户交互体验有一定影响

## 六 引用
  1. Brown, D.G., Golod, D. Decoding HMMs using the k best paths: algorithms and applications. BMC Bioinformatics 11, S28 (2010). https://doi.org/10.1186/1471-2105-11-S1-S28

## 七 致谢

感谢曹明隽同学在前端方面的指导。